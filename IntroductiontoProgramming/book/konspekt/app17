У каждого проекта есть свойство расти и развиваться. Не все, но многие проекты достигают того момента, когда запросы к Базе Данных начинают работать медленнее. Исчезает та легкость и простота работы с БД и приходит время оптимизаций.

Рассмотрим простой пример: в нашем Rails-приложении есть модель User. У этой модели есть поле full_name, которое заполнено стандартно: "Имя Фамилия". По условию задачи, нам необходимо перенести поле full_name в модель Profile и разделить его на два соответствующих поля: first_name и last_name.

Для решения этой задачи вполне логичным будет написать rake task, который заполнит поля first_name и last_name соответствующим образом. Но есть одна проблема - нашему проекту уже много лет и у нас сотни миллионов пользователей.

Рассмотрим следующий пример:

namespace :profiles do
  task :populate_first_and_last_name do
    User.all.each do |user|
      first_name, last_name = user.full_name.split(' ')
      Profile.create(user_id: user.id, first_name: first_name, last_name: last_name)
    end
  end
end
С этим примером есть огромная проблема, а именно этот кусочек кода: User.all. Это приводит к следующему запросу:

SELECT `users`.* FROM `users`
Что на больших обьемах данных может стать критичным для сервера.

Нужно найти способ выбирать данные частями.

Как всегда, Rails уже позаботился об этом и предоставляет метод find_in_batches.

find_in_batches все так же выбирает данные на основе вашего запроса, но делает это частями. Это не позволяет "завалить" сервер одним запросом. По-умолчанию размер каждой части равен 1000 записей. Если переписать rake task с использованием find_in_batches, получается такой код:

namespace :profiles do
  task :populate_first_and_last_name do
    User.find_in_batches.each do |users|
      users.each do |user|
        first_name, last_name = user.full_name.split(' ')
        Profile.create(user_id: user.id, first_name: first_name, last_name: last_name)
      end
    end
  end
end
Сначала мы выбираем первые 1000 пользователей, записываем переменную users, проходим по ней, заполняем поля first_name и last_name, далее следующая тысяча. Что самое главное, запросы при этом выглядят вот так:

SELECT  `users`.* FROM `users`  ORDER BY `users`.`id` ASC LIMIT 1000
Как видим, сортировка производится по id и устанавливается лимит в тысячу записей.

Если мы хотим изменить лимит, мы можем явно передать параметр batch_size:

User.find_in_batches(batch_size: 2000) do |users|
  # more code
end
Такой подход позволяет значительно сократить нагрузку на сервер и выполнить таск гораздо быстрее. Но есть еще один момент, на который следует обратить внимание - это Profile.create. Наш таск сделает столько же INSERT-запросов в Базу Данных - сколько пользователей мы имеем. А их у нас миллионы.

В данном случае нам на помощь приходит gem activerecord-import. Он позволяет импортировать много записей в БД одним INSERT запросом. В документации к нему есть отличные примеры использования.

С этим gem'ом rake task будет выглядеть вот так:

namespace :profiles do
  task :populate_first_and_last_name do
    User.find_in_batches.each do |users|
      profiles = []
      users.each do |user|
        first_name, last_name = user.full_name.split(' ')
        profiles << Profile.new(first_name: first_name, last_name: last_name)
      end
      Profile.import profiles, validate: false
    end
  end
end
Мы все так же выбираем юзеров частями, но теперь мы формируем массив profiles, который в конце цикла по тысяче пользователей делает один INSERT, который в свою очередь создает тысячу профайлов.

Для следующей тысячи пользователей повторяется тот же алгоритм.

Давайте подумаем, что нам дает подобный подход: Во-первых, мы не уроним сервер тяжелым запросом User.all. Во-вторых, если бы пришлось делать find_each вместо User.all, то это сулило бы нам N*2 количеством запросов. Например, при 100 миллионах пользователей - мы бы получили 200 миллионов запросов при миграции. Так, как один запрос нам нужен чтобы вытащить данные о пользователе, а второй - чтобы создать для него профиль.

При использовании find_in_batches и activerecord-import, мы сократили время выполнения таска и количество запросов в разы. 100 миллионов пользователей мы перенесем всего за 200 тысяч запросов.

Имея дело с большими обьемами данных, всегда приходится искать варианты ускорения выполнения запросов, миграций и rake task'ов. find_in_batches идеально подходит для решения подобных задач.